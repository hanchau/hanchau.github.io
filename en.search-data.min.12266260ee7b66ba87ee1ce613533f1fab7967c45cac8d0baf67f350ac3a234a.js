'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/posts/','title':"Blog",'content':""});index.add({'id':1,'href':'/about/','title':"About",'content':" Hi there, I am Anuj, first of all, thanks for visiting the page. It\u0026rsquo;s an honor to serve the community, I am currently working as a Data Scientist/Engineer with India’s leading AgriTech Startup Agrostar. Last year I graduated from the Indian Institute of Technology Kanpur - A Great Place to Learn, Now I am living in Viman Nagar, Pune with my 2 colleagues/batch-mates. My philosophy of life is to solve problems and also provide a helpful environment to others so that we all can manage conflicts and move in better directions :).\nI\u0026rsquo;ll try my best to keep the content more implementation focused. My current areas of experience are Data Pipeline Architectures and of course automation to get a good night\u0026rsquo;s sleep in the mundane routines.\n "});index.add({'id':2,'href':'/cheetsheet/','title':"Cheet Sheet",'content':" coming soon\u0026hellip;\n "});index.add({'id':3,'href':'/connect/','title':"Connect",'content':"Github   Linkedin    GMail   Facebook   Twitter    "});index.add({'id':4,'href':'/posts/understanding_hyperloglog/','title':"HyperLogLog!",'content':"Cardinality Quoting wikipedia, the Cardinality is nothing but \u0026ldquo;the number of elements in a given set\u0026rdquo;.\nCardinality Estimation Algorithm In the last post we discussed CAP theorem and saw why it becomes hard to make systems Available and Consistent when there are potential partitions in the systems. I was thinking about my next post back then and finalized Consistent Hashing, But then I came across an idea to first write about HyperLogLog because of its Elegance and Power.\nLet\u0026rsquo;s jump into it. I\u0026rsquo;m just gonna throw a problem at you.\nWe have a stream of elements and we want to maintain the count of distinct elements. How can we solve it? Before proceeding please give it some thoughts. Few hints: Can we do it in Run time. Think of bloom filters. Think of approximating it. Think of upper bounding it, cuz every finite set must have finite distinct elements, right?  Initially, All of this started with counting, The above example is just an abstraction. The real use case would be like -\nI have a webpage and I want to maintain a counter of the distinct IPs that have landed on the page. (Also the page is distributed over multiple servers) Let\u0026rsquo;s start solving it.\nFamework We\u0026rsquo;ll take these points into consideration when we\u0026rsquo;ll solve the problem with multiple approaches -\n Space Complexity Time Complexity Estimation error  Some fundamental operations are -\n Load(x) : Store x in the memory. Lookup(x, ds) : lookup x in the ds data structure (memory). get_new_elem() : get a new element. len(L) : return the length of a L which can either be a list of a Map. dump(x, d) : dump the x data into the d data store (disk). hash(x) : hash x into a 6 bit binary hash. e.g. hash(\u0026ldquo;A\u0026rdquo;) = 001011, hash(\u0026ldquo;B\u0026rdquo;) = 001001 f_k_bits(x) : get the first k bits of binary x. e.g. f_3_bits (00101101) = 001, f_5_bits (00101001) = 00101 l_k_bits(x) : get the last k bits of binary x. e.g. l_3_bits (00101101) = 101, l_5_bits (00101011) = 01011 rho(x) : return the position of first 1 bit from left of binary x. e.g. rho(001101) = 3, rho(010101) = 2 Max_bits(x,y): return the maximum between 2 binary strings. Sum(L) : return the sum of all elements of the list L.  Some Assumptions are -\n We\u0026rsquo;re getting a stream of elements.  Approach 1: Store the elements in the disk. Whenever a new element comes, dump it into the big data store, and rerun the count distinct alogrithm -\n1. def distinct(L): 2. counter = 1 # loop 1 3. for i in range(1,len(L)): # loop 2 4. for j in range(0,i): 5. if L[i]==L[j]: 6. break 7. if i==j+1: 8. counter+=1 9. return counter 10. 11. def main(): 12. while True: 13. new_elem = get_new_elem() 14. dump(new_elem, D) # Space Complexity = O(n) 15. L = Load(D) # Time Complexity = O(n^2) 16. crdnlty = distinct(L) 17. 18. main() Approach 1: It is a trivial approach. It stores the elements into the disk and it has to load the elements into the memory every time it needs to calculate the Cardinality.\n   Space Complexity Time Complexity Estimation Error %     O(n) O(n^2) 0    Approach 2: Maintain a Hash Map 1. def distinct(new_elem, _hMap): # Lookup in Hashmap = O(1) 2. if Lookup(new_elem,_hMap): 3. _hMap[new_elem] += 1 4. else: 5. _hMap[new_elem] = 1 6. return len(_hMap) 7. 8. def main(): # A Hash Map, Space C - O(n) 9. _hMap = {} 10. while True: 11. new_elem = get_new_elem() 12. distinct(new_elem, _hMap) # Time Complexity = O(1) 13. crdnlty = distinct(new_elem,_hmap) 14. 15. main() Approach 2: Maintain a hashmap in memory. So whenever a new element comes, it looks up the element in the map. If the element exists then it updates the count of the element. If the element doesn\u0026rsquo;t exist then it adds the element in the map and initializes its count with 1. The length of the map is the Cardinality of the set which we are looking for.\n   Space Complexity Time Complexity Estimation Error %     O(n) O(1) 0    Approach 3: LogLog Estimation 1. def addToLogLog(new_elem, LL): 2. hash_elem = hash(new_elem) 3. frst_2_bits = f_2_bits(hash_elem) 4. last_4_bits = l_2_bits(hash_elem) 5. old_max = LL.get(frst_2_bits) 6. rho_elem = rho(last_4_bits) 7. new_max = Max(old_max, rho_elem) 8. LL.update( {first_2_bits: new_max}) 9. 10. def getLLEstimate(LL): 11. sumLL = Sum(LL.values()) 12. lenLL = len(LL) 13. arith_mean = sumLL/lenLL 14. LLEstimate = 2**arith_mean 15. return LLEstimate 16. 17. def main(): # Hash Map of Space Cmplxty - O(m) # `m` is the number of buckets 18. LL = { '00':0, '01':0, 19. '10':0, '11':0 } 20. 21. while True: 22. new_elem = get_new_elem() 23. addToLogLog(new_elem, LL) # Time Complexity = O(m) 24. crdnlty = getLLEstimate(LL) 25. 26. main() Approach 3: A lot is going on in the third approach. Let\u0026rsquo;s get the intuition first, and then we can walk through an example to get the Cardinality of a stream using this method.\nIntuition: The idea behind LogLog Estimation is to\n Map the arbitrary stream to a uniformly distributed hash function, Observe the bit pattern of the hash seen so far, Make an Estimation attempt assuming the uniform distribution.  In other words\n Let\u0026rsquo;s say We got an element - \u0026ldquo;A\u0026rdquo;, We hashed it i.e. hash(\u0026ldquo;A\u0026rdquo;) - \u0026lsquo;001010\u0026rsquo; (assumption), The rho(x) in the algorithm is the bit pattern, rho(\u0026lsquo;001010\u0026rsquo;) = 3 : the position of first 1 bit from left, Estimation attempt: Cardinality = 2^3 i.e. 8 The Estimation is wrong as we have only seen \u0026ldquo;A\u0026rdquo; so far. Continue Reading for an explanation :).  Park the above Estimation aside for a while, I know you must have some questions like - from where this exponent is coming into the picture, why did we hash our elements, etc.\nQuoting from the LogLog Paper -\nFor a string x ∈ {0,1}∞, Let ρ(x) denotes the position of its first 1-bit. Thus ρ(1\u0026hellip;) = 1, ρ(001\u0026hellip;) = 3, etc. Clearly we expect about n/2ᵏ amongst the distinct elements of the set to have a ρ value equal to k.  Let's look at it this way : Part A: Consider 2 binary strings - A = '1 _ _ _ _ _' and B = '0 _ _ _ _ _'. We can see that the first bit divides the set of all possible strings of length 6 into two halves. Similarly the first 2 bits divide the set of strings into 4 quarters. That's why we use an exponent 2. Part B: The large value of the ρ indicates a large number of distinct elements. Because of the uniform distribution of the hash function, we can say that the large ρ implies the high probability of having a large number of distinct elements. Part C: Our Estimation was wrong because the paper clearly states that the algorithm gives asymptotic good results i.e. as the number of elements in the stream reaches ∞, the estimation error reaches 1.30/√m where m is the number of buckets. Part D. In our algorithm we've used 6-bit hash function, and bucket size of 4, but the usual numbers are 32-bit or 64-bit hash functions and 1024 or 2^10 buckets. The paper talks about many design decisions for optimizations. Example Walkthrough: Stream = [\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;, \u0026lsquo;d\u0026rsquo;, \u0026lsquo;e\u0026rsquo;, \u0026lsquo;f\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;, \u0026lsquo;e\u0026rsquo;].\nHash of stream = [101110, 100010, 010001, 110010, 010010, 001001, 100010, 010001, 010010]\n   HLL buckets elem ρ     00:0, 01:0, 10:0, 11:0 10 11 10 1   00:0, 01:0, 10:1, 11:0 10 00 10 3   00:0, 01:0, 10:3, 11:0 01 00 01 4   00:0, 01:4, 10:3, 11:0 11 00 10 3   00:0, 01:4, 10:3, 11:3 01 00 10 3   00:0, 01:4, 10:3, 11:3 00 10 01 1   00:1, 01:4, 10:3, 11:3 10 00 10 3   00:1, 01:4, 10:3, 11:3 01 00 01 4   00:1, 01:4, 10:3, 11:3 01 00 10 3   00:1, 01:4, 10:3, 11:3 - -    Now the Estimate is ~ 2^(11/4) i.e. 2^2.8 ~ 7 which is a pretty good estimate as we have 6 distinct elements in our stream.\n   Space Complexity Time Complexity Estimation Error %     O(m) few KBs O(m) constant 1.30/√m    Cardinality Estimation over a distributed system. We can see that Approach 1 and Approach 2 are non-scalable solutions. If we increase the number of elements in the stream to 2^32, then (assuming each element takes k bytes) we\u0026rsquo;ll need a memory of size 2^32*k bytes which are roughly K GBs. On the contrary, the LogLog uses few KiloBytes of Memory. In a distributed system, the Total Cardinality of the streams seen by all the components of the system can be calculated as - Assuming there are D machines in a distributed system. Each machine maintains its LogLog Data structure as given below.\nHLL_1 = {x1: a11, x2:a12, x3:a13, .. , xk:a1k} HLL_2 = {x1: a21, x2:a22, x3:a23, .. , xk:a2k} . . . HLL_D = {x1: aD1, x2:aD2, x3:aD3, .. , xk:aDk} We can easily merge them into HLL_Total by taking a component-wise maximum of all elements i.e. HLL_Total = {x1: max(a11, a21, ..., aD1), x2: max(a12, a22, ..., aD2), ... xD: max(a1k, a2k, ..., aDk)}  Thus we can get the Cardinality Estimation of a distributed system with similar Time and Space Complexities.\nWhy named LogLog?? I think by now you must\u0026rsquo;ve got the idea why it is named LogLog. Assuming we are counting till 2^32 ( log2^32(base 2) = 32). Assuming 2^10 i.e. 1024 number of buckets. We are left with 22 digits (after 10 digits) in a 32-bit binary string. To store the ρ of 22-bit string we only need log(22) bits because in a 22-bit string the maximum possible value of ρ can be 22 and this number can be stored in the memory using log(22) bits i.e. ~ 5. This is the reason why the Space Complexity of this algorithm is constant (a few KBs).\nSome Notes on SuperLogLog and HyperLogLog. SuperLogLog: The LogLog paper talks about a few engineering optimizations like Truncation Rule and Restriction Rule which when combined with the LogLog algorithm yields a new algorithm, SuperLogLog\n Truncation Rule: This rule simply states that when we are calculating the arithmetic mean of the HLL map, just drop the top k % values. The optimal value of k is found to be 30% for the best estimation.  If HLL_1 = {x1: a1, x2:a2, x3:a3, .. , x100:a100} (sorted on a), then drop 0.3k elements from the top i.e. HLL_1 = {x31: a31, x32:a32, x33:a33, .. , x100:a100}\nRestriction Rule: This rule bounds the number of buckets to be used.  HyperLogLog: The HyperLogLog uses a Harmonic mean instead of Geometric Mean. If HLL_1 = {x1: a1, x2:a2, x3:a3, .. , x100:a100} then the estimation becomes ~ ( 2^-a1 + 2^-a2 + 2^-a3 + \u0026hellip; + 2^-a100) / m^-1.\nConclusion Having LogLog Cardinality Estimation in our bag of algorithms now we can estimate cardinalities without worrying about space. Our webpage now can be hosted on a machine having few gigs of memory with maintaining the count of distinct users that have landed on the page. With the emergence of IoT the number of IPs will grow like crazy, from IPv4 to IPv6. The HyperLogLog will save the day.\nThanks, Cheers \u0026amp; Happy Glogging.\nReferences  The Original LogLog Paper The HyperLogLog Paper: An improvement on LogLog Google\u0026rsquo;s Paper on furthur improvements  Cool Videos to watch.       "});index.add({'id':5,'href':'/posts/understanding_cap_theorem/','title':"CAP theorem!",'content':"Introduction In the last post I discussed why we need distributed systems and how to setup gearman on a cluster. In this post I\u0026rsquo;ll try to convince you that CAP theorem is indeed a \u0026ldquo;theorem\u0026rdquo;. Anyways, We like distributed systems!! (because they provide us the features that we really want) but\u0026hellip;, Are they trivial to implement?.\nWords of Wisdom!! My colleague once said\nMore code =\u0026gt; More Complexity =\u0026gt; More Monster Bugs =\u0026gt; More Pain  Fundamental Operations Let\u0026rsquo;s start with that, Initially we used to have a beautiful piece of machine having 8 cores, 16 gigs of memory, few hundread gigs of storage. So if you wanted to get a state (An abstraction over the data stored in the memory or in disk), you can do\nA.getState() and if you want to set the state you can similarly do\nA.setState(10) So these are the fundamental operations that you can do to a state.\nSingle Machine System. If you have a stream of operations, you can always maintain a queue and do the following-\n1. while !empty(Q): 2. op = dequeue(Q) 3. if validate(op): 4. response = exec(op) 5. send_back(response) 6. else: 7. response = op_not_found_error() 8. send_back(response) Consistency vs Availability Available System. In the above example we can see that the machine is Consistent, it either gives you the recent write (because all the operations are sequential) or the error. We have a single source of truth here (our beautiful machine\u0026rsquo;s state). The Availability of the machine depends on the network and the machine itself. Now, we want to imporve the Availability of the machine. If we add one more machine to the system and want to implement the same queue Q that we did for a single machine we can do the following.\n 1. def exec_on_machine(op, machine_name): 2. if validate(op): 3. response = exec_mac(op, machine_id) 4. send_back(response) 5. else: 6. response = op_not_found_error() 7. send_back(response) 8. 9. 10. while !empty(Q): 11. op = dequeue(Q) 12. if check_machine_A_online(): 13. exec_on_machine(op, A) 14. sync_states(A,B) 15. else: 16. exec_on_machine(op, B) 17. sync_states(B,A) Hurray!! We implemented a more availabile system, but\u0026hellip;. Consider the following scenario:\nGiven a queue of operation requests - Q = object1.setState(10) | object1.setState(15) | object1.getState()    Op. No. Online Offline A\u0026rsquo;s state B\u0026rsquo;s State     1 A, B - 10 10   2 A B 15 10   3 B A 15 10    Above table shows the state of both machines. We can see that the recent write is 15. The state of B is still 10 which is a not a recent write. Due to the down time of B during the second operation it never synced with the machine A. The third operation will not get the old state not the recent write. Hence our system becomes Availabile but Inconsistent.\nConsistent System. Now let\u0026rsquo;s say we don\u0026rsquo;t want any inconsistency in the system and for that we make a policy that if all the machines are not online then just don\u0026rsquo;t execute any operation, which can be implemented as follows:\n1. def exec_on_machine(op, machine_name): 2. if validate(op): 3. response = exec_mac(op, machine_id) 4. send_back(response) 5. else: 6. response = op_not_found_error() 7. send_back(response) 8. 9. 10. while !empty(Q): 11. op = dequeue(Q) 12. if check_machine_A_online() and check_machine_B_online(): 13. # added a get_location condition to show 14. # the significance of 2 machines. 15. if get_location()==\u0026quot;Pune\u0026quot;: 16. exec_on_machine(op, A) 17. sync_states(A,B) 18. else: # machine in Mumbai 19. exec_on_machine(op, B) 20. sync_states(B,A) 21. else: # enqueue the dequeued op request 22. enqueue(Q,op) Now consider a scenario:\nGiven a queue of operation requests - Q = object1.setState(10) | object1.setState(15) | object1.getState()    Op. No. Online Offline A\u0026rsquo;s state B\u0026rsquo;s State     1 A, B - 10 10   2 A B 10 10   3 B A 10 10    We can see that second and third request will not be executed because one of the machine is offline in both the cases. That means our systems is Consistent but it is not as Availabile as it was previously.\nFormal Definition I think you got the point I\u0026rsquo;m trying to make. So let\u0026rsquo;s copy the definition from the wikipedia which says\nIt is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: Consistency: Every read receives the most recent write or an error Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes Conclusion When we headed towards a distributed system from our beautiful machine we realized that we can either promise Availability of the system or Consistency in the system given a Partition occurs.\nEnding the post with few more words of wisdom.\nYour Power is Your Weakness, at least for the distributed systems  Thanks, Cheers \u0026amp; Happy Glogging.\n"});index.add({'id':6,'href':'/posts/understanding_distributed_systems/','title':"Distributed systems!",'content':"What are Distributed Systems? In the last post I gave a brief about the concept of parallel processing through some examples. Now we know how to make our systems more parallel in nature, I think it\u0026rsquo;s time learn the concept of distributed processing, how it\u0026rsquo;s different from parallel processing and how to integrate it in our existing projects. There are lots of definitions of Distributed System but I\u0026rsquo;ll give you the one I like the most which is obviously mine (HAHA).\nA system having a set of computer systems located at different geographical locations appears as a single unit to the end user.  Examples of Distributed Systems Some examples of distributed systems are:\n The Internet Google Facebook  Primitive Features There are some useful primitive features to exploit out from a distributed system:\nDistributed Storage. Distributed Computing. Distributed Memory. High Level Features Now from these primitive features we can construct many high level features having varied quantities of primitives features.\n No Single point of Failure. High Scalability. Low Latency. Reliability.  Single Point of Failure. Now we know how to process parallelly, let\u0026rsquo;s say you wrote an application which uses all these cool parallel processing features and deployed it on the server. Days passes and one day a thunderstorm comes and electricity system of the city goes down. Nobody can use ur application. So\u0026hellip; what to do?\nI think you got the point, there should not be a single thing in our system on which the whole system depends i.e. if that thing goes down system goes down.\nOne solution is, why don\u0026rsquo;t we replicate our system in another server who has a different electricity supplier. Well it is possible.\nWe can go with this solution and to achieve no single point of failure with gearman we just have to create a new replica of the original server.\nThe Gearman solution:\nGiven initial server A. Get an new server B in a seperate geographical location. Clone your application. Start a new gearman job server. $ gearman -d \u0026lt;port\u0026gt; Add all your workers/clients to both the gearman job servers A and B. Daemonize your workers on B. Now, if any one of them goes down there will be another one to process the tasks. Hurray!! Now we\u0026rsquo;re safe from any kind of electricity isssue.\nNOTE: By getting servers of different electricity supplier we\u0026rsquo;re just trying to minimize the probability of getting a downtime, because as we keep on increasing the number of backup servers from different electricity suppliers the probability of having an electricity shutdown for all the supplier at the same time would be very less.  High Scalability I think of scaling as an increase in the number of tasks to perform by the system. These tasks can be CPU Intensive, I/O Intensive or can have both CPU and IO bursts. So, let\u0026rsquo;s say you have a live video streaming application which was initially doing 10^1 taks/min with a machine limit of 10^2 tasks/min, but due to sudden increase in the user base because a live stream suddenly got viral, your system now receives 10^3 tasks/min. So\u0026hellip; What will you do? Well you might say that this is not practical, but trust me if it happens, given you just started up as a live video streaming site.\nThe Gearman solution:\nGiven initial server A. Get an extra server B. Clone your application. Start a new gearman job server. $ gearman -d \u0026lt;port\u0026gt; Add all your workers/clients to both the gearman job servers A and B. Daemonize your workers on B. Repeat this for 8 new machines. Maximum system limit becomes 10 * 10^2 = 10^3 tasks/min.\nHurray!!, Now we can sleep like a baby. Both of the other features are similar but with their own little flavor like, if you want your application response to be geography agnostic i.e. A person in India gets response in 100 ms and A person in US also gets response in the same time, Then you have to deploy it on multiple continents with all of its dependencies like DBs, Metadatas and Caches.\nThanks, Cheers \u0026amp; Happy Glogging.\nA Note on COVID-19 pandemic: Keep Calm, Eat Well \u0026amp; Stay @ Home.  Cool videos to watch Brief about the downsides of distributed systems.       A more detailed version.    "});index.add({'id':7,'href':'/posts/understanding_parallel_processing/','title':"Parallel processing!",'content':"Why Parallel Processing?. Gearman Job Server is the first parallel/distributed processing framework that I\u0026rsquo;ve learned/worked on which is indeed a very old framework out there. One of the things I like about Gearman is that it\u0026rsquo;s very generic and you can play around a lot. In the last post I wrote how to setup the gearman job server and in this post I\u0026rsquo;m going to brief you why we need parallel processing with some examples:\nExample 1: Divide and conquer. Let\u0026rsquo;s say we have 10000 colored balls having red, green and blue colors and 3 buckets of each color. The task is to segregate the balls to the respective colored buckets.\n1 Approach: You pick a ball, look at the color, put it inside the same colored bucket and repeat. Time Taken: Assuming T to be the time taken to segregate one ball, total time would be (10000*T).\n2 Approach: Call K number of friends if possibles, Get K number of beers, Divide the balls among y\u0026rsquo;all so that each one of you will process 10000/K number of balls only, Everyone follow the I Approach. Time Taken: Assuming T to be the time taken to segregate one ball, total time would be (10000*T/K).\nExample 2: Resource Optimizion. Again, Let\u0026rsquo;s say we have 10000 balls and 3 buckets A, B, C with A having clean water, B having soap water, C having clean water. The task is to clean all the balls pun intended :D.\n1 Approach: You pick a ball, put it inside A, then B and then C and repeat for all the balls. NOTE: when you are using one bucket, others are not in use or idle.  Time Taken: Assuming T/3 to be the time taken to process a ball through one bucket, total time would be ((T/3 + T/3 + T/3)10000) ~ (10000*T) .\n2 Approach: Call your two best friends Alex and Bob, Assign each task of using bucket A, bucket B, bucket C to y\u0026rsquo;all. Now what you will do is just pick a ball, dip it in bucket A and give it to Alex and repeat. Alex will dip it in bucket B and will pass the ball to the Bob and repeat. Bob will dip it in bucket C and Voilla, One ball is cleaned. Time Taken: Assuming T/3 to be the time taken to process a ball through one bucket, but now there\u0026rsquo;s a catch\nNOTE: when you are using one bucket, others are being used as well. After everty T/3 time, one ball is getting cleaned which makes the effective time taken to clean a ball is T/3 +- some error;  total time would be ~ (10000*T/3).\nSo, I think I\u0026rsquo;ve convinced you that parallel processing is gooood.\nNOTE: Parallel processing can only be done with some specific types of tasks. Some tasks are inherently sequential. A fine example would be making a cup of tea, you have to follow the steps. One way to identify if a task can be parallelized or not is that if there is a some sort of repetition or lack of system resource utilization (like in example 2) then you can think of making the task into sub tasks and processing them parallelly.  How Gearman helps you achieve that?. In the above examples we saw how we can do tasks in parallel ways. Let\u0026rsquo;s say we have a task which can be divided into K subtasks. To complete the task what we can do in gearman is create a worker which will process one subtask at a time. Now just instantiate P number of workers and submit all the K tasks to the gearman job server which will keep them in a queue. What you will see is that K number of subtasks are being processed parallelly i.e. every worker is processing one subtask. Hurrray!! We just processed our task parallelly!!\n"});index.add({'id':8,'href':'/posts/gearman_to_the_rescue/','title':"Gearman to the Rescue!",'content':"This article offers a setup of Gearman for Parallel/Distributed processing.\nIntroduction An elegant and generic application framework for parallel and distributed processing.\nSetup  Setup gearadmin and gearman with.  MacOS $ brew install gearmandLinux $ apt install gearman-job-server  Setup gearman job server for mac.\n  You can get the gearman python package.\n  Copy the gearman package as shown and install the requirements.\n   $ source /path/to/env $ python -m site $ cp -r \u0026lt;path/to/gearman/package/dir\u0026gt; \u0026lt;/path/to/env\u0026gt;/lib/python3.8/site-packages/ $ pip install -r requirements.txt To check the proper installation do   $ gearadmin --server-version $ gearadmin --status To check the proper installation of the python gearman package do   $ python -c \u0026quot;import gearman\u0026quot; if no error is thrown then you are good to go.\nGearman Architecture A web cast to get a good idea about the architecture of the Gearman.\n   "});})();